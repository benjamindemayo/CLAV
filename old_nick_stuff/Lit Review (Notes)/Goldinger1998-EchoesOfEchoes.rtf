{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 Most theories regard idiosyncratic aspects of speech as noise to be filtered in perception, but episodic theories (what, what) suggest the notion that perceptual details, idiosyncratic and all, are stored in memory as well and are in fact integral to later perception. Data and theory from this work suggest that detailed episodes constitute the basic substrate of the mental lexicon.\
\
You can begin to see phoneme categorization in acquisition as a function of how the lexicon matching parses the speech stream while listening to language from speakers.\
\
Exemplar theory: assumes that every experience, such as perceiving a spoken word, leaves a unique memory trace. On presentation of a new word, all stored traces are activated, each according to its similarity to the stimulus. The most activated traces connect the new word to stored knowledge, the essence of recognition.\
\
To Semon and Galton, abstraction occurs during retrieval as countless partially redundant traces respond to an input. Semon\'92s theory went away for a while but found a resurgence under the emergence of cognitive science. Goldinger is all about returning to an episodic view, yo.  Specifically in regards to spoken word perception, production, and memory. This paper tests specifically the MINERVA model.\
\
Great acoustic variability arises in nominally identical words across speakers. Nevertheless, listeners typically understand new speakers instantly. Even Joos thought that phonetically irrelevant voice information is filtered away in perception. Speaker normalization would presumably allow listeners to follow the lexical-semantic content of speech, with superficial details exploited by the perceptual machinery and then discarded.\
\
In a lexicon containing myriad and detailed episodes, new words could be compared directly with prior traces. By this view, speaker normalization becomes a testable hypothesis, rather than an assumed process, equally evidenced by positive or null effects.\
\
Many contemporary models resemble Semon\'92s theory, positing parallel access to stored traces, motivated by common findings of memory for \'91surface\'92 details of a wide variety of experiences. Jusczyk does a lot of work on developmental models of speech perception incorporating episodic storage and on-line abstraction.\
\
Jacoby and Hayman (1987) suggested that printed word perception relies on episodic memory; it would be surprising if spoken word perception operated differently. Compared to fonts, voices are more ecologically valuable and worthy of memory storage.\
\
Linguistic processes often create lasting detailed memories. People spontaneously remember the presentation modalities of words, the spatial location of information in text, and the exact wording of sentences.\
\
Human voices convey personal information, such as speakers\'92 age, sex, and emotional state (Abercrombie, 1967). These aspects of speech are typically ignored in perceptual and linguistic theories, but they are clearly important.\
\
Voice encoding seems to be automatic. Performance in different-voice trials was affected by the perceptual distance between study and test voices, suggesting that study traces retain voice details with great precision. Voice effects diminished over time, however, and were absent after 1 week. In a similar implicit memory experiment, however, reliable voice effects were observed at all delays.\
\
If episodic traces of words persist in memory and affect later perception, might they constitute the mental lexicon? By the time this paper came out, little formal modeling of the lexicon had occurred.\
\
MINERVA 2 is an extreme episodically \'91pure\'92 model in that it assumes that all experiences create independent memory traces that store all perceptual and contextual details (Hintzman, 19986, 1988). In MINERVA 2, despite their separate storage and idiosyncratic attributes, aggregates of traces activated at retrieval create behavior. Prior testing of MINERVA 2 produce behaviors typically considered hallmarks of abstract representations, such as long-lasting prototype effects in dot-pattern classification and memory.\
\
In MINERVA 2, an aggregate of all activated traces in memory constitutes an echo sent to working memory from LTM. The echo may contain information not present in the probe, such as conceptual knowledge, thus associating the stimulus to past experience. Echo intensity reflects the total activity in memory created by the probe. ECHO INTENSITY increases with greater similarity of the probe to existing traces, and with greater numbers of such traces. Thus, it estimates stimulus familiarity and can be used to simulate recognition memory judgments. Assuming that stronger echoes also support faster responses, inverse echo intensities were used to simulate response times (RTs) in the present research. ECHO CONTENT is the net response of memory to the probe. Because all stored traces respond in parallel, each to its own degree, echo content reflects a unique combination of the probe and the activated traces.\
\
If a myriad of detailed spoken traces of words reside in LTM \'97> if a common word is presented in a familiar voice, many traces will strongly respond. Thus, even if a perfect match to the probe (stimulus? input?) exists in memory, all of the similar activated traces will force a \'91generic echo\'92 \'97 it\'92s central tendency will regress toward the mean of the activated set. However, if a rare word is presented in an unfamiliar voice, fewer traces will (weakly) respond. Thus, if a perfect match to the probe exists in memory, it will clearly contribute to echo content. Therefore, token repetition effects should be greater for unusual words or for words presented in unusual contexts.\
\
MINERVA 2 even qualitatively replicates the recognition memory data from Goldinger (1996). When the model\'92s lexicon is created, every input creates a new trace; some forgetting occurs over time, however, simulated by random elements reverting to zero (determined stochastically over forgetting cycles).\
\
The dependent variable was echo intensity, as he manipulated different inputs to the artificial lexicon. The model also provided a new prediction that Goldinger (1996) didn\'92t have; he provided word inputs with varying frequency this time. Found a new result where, due to the frequency manipulation, the same-voice advantage diminished as word frequencies increased. Subtracted mean SV minus DV trials in echo-intensity units. High-frequency words activated many traces, so the details of any particular trace (even a perfect match to the new token) are obscured in the echo. Old high-frequency (HF) words inspire \'91abstract\'92 echoes, obscuring context and voice elements of the study trace. \
\
Conducted a post hoc correlation analysis with Goldinger (1996) data, which confirmed stronger voice effects among lower frequency words (r = -.35, p < .05).\
\
Single-word shadowing task: repeat the word as soon as you hear it; dependent measure is the latency between response stimulus and response onsets. Porter and Lubker (1980) showed that listeners could shadow syllables faster in a choice RT procedure than they could press a button in the same task, suggesting that shadowers may drive their articulators directly from speech input.\
\
Preschool children and adults lake track stimulus word durations in shadowing \'97 wont to repeat it in the way they heard it; there\'92s something about the voice that\'92s important to the encoding.\
\
Goldinger now wants to say this shadowing data reveals something about lexical representation, at least in the context of MINERVA 2.\
\
RTs should decrease as repetitions increase, imitation should increase as repetitions increase, and frequency effects should decrease with increasing repetitions. The model predicts a frequency by repetition interaction in both dependent measures \'97 imitation and RT.\
\
As Hintzman noted, although MINERVA 2 is a quantitative model, it is best suited for qualitative analysis. If it predicts the major trends of the data, the model may constitute a reasonable account.\
\
The present findings suggest an integral role of episodes in lexical representation (Jacoby & Brooks, 1984). Prior research has shown that detailed traces of spoken words are created during perception, are remembered for considerable periods, and can affect later perception, data most naturally accommodated by assuming that the lexicon contains such traces. The present study extends such prior research, showing episodic effects in single-word and nonword shadowing. Specifically, a strict episodic model produced close qualitative fits to the data. This provides some validation of the multiple-trace assumption.\
\
Joos never suggested that information was lost by speaker normalization, but this was assumed by later theories, with voice details considered to be noise to be resolved in phonetic perception (Pisoni, 1993).\
\
No evidence shows that normalization reduces information. Several models show that normalization and voice memory can peacefully coexist. But is normalization theoretically necessary? Most theories treat it as a logical necessity because variable signals must be matched to summary representations. But an episodic lexicon should support direct matching of words to traces, without normalization. So normalization can\'92t really be part of the system if we assume an episodic lexicon.\
\
MINERVA 2 makes the extreme assumption of numerous, independent memory traces. Because the present goal was to assess the viability of an episodic lexicon, this unwavering assumption was desirable. Second, it makes simultaneous predictions regarding echo intensity and content, which naturally conform to the dependent measures in shadowing (RTs and speech acoustics).\
\
In hybrid models, both abstract lexical codes and episodic traces contribute to perception. Episodes mediate token-specific repetition effects, but abstract codes provide the lexicon stability and permanence.\
\
Distributed models: ************\
Another alternative to pure episodic models are distributed models. In McClelland and Rumelhart\'92s (1985) model, memory traces are created by activation patterns in a network. The trace for each stimulus is unique and can be retrieved by repeating its original pattern. The model develops abstract categories by superimposing traces, but its storage is more economical than MINERVA 2. To McClelland and Rumelhart, \'91our theme will be to show that distributed models provide a way to resolve the abstraction \'97 representation of specifics dilemma. With a distributed model, the superposition of traces automatically results in abstraction though it can still preserve to some extent the idiosyncrasies of specific events and experiences.\'92\
\
The distributed model presents a reasonable compromise between episodic and abstract models. For example, it is easy to imagine how distributed networks derive central tendencies from exemplars. However, with all memory traces superimposed, it is unknown whether distributed models could display adequate sensitivity to perceptual details, as in the present data. Can repetition of an old word have a \'93special\'94 effect after many similar words are combined in a common substrate? Presumably, if contextual encoding sufficiently delimits the traces activated during test (as in MINERVA 2), such results are possible. *************\
References:\
-Pisoni, 1993 \'97 voice details considered to be noise to be resolved in phonetic perception}