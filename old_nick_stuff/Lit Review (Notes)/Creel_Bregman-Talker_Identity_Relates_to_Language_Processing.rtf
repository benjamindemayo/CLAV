{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 Speech perception itself may arise from phylogenetically earlier vocal recognition. This discusses evidence that many cues to talker identity are also cues to speech-sound identity. Rather than brushing talker differences aside, explicit examination of the role of talker variability and talker identity in language processing can illuminate our understanding of the origins of spoken language, and the nature of language representations themselves.\
\
Speech can refer to things, but it also identifies or classifies the person speaking; ergo, speech\'92s dual function in this way links language to other types of vocal communication systems \'97> so recognizing speech and recognizing speakers are intertwined.\
\
In a modular view of speech perception, you use the elements of the acoustic signal that help you differentiate the word that the speaker is saying and discard the rest; however, details such as vocal pitch may influence comprehension because they indicate the speaker\'92s identity and can refine the listener\'92s expectations about what particular speakers are likely to talk about.\
\
Two theories of language processing with different accounts of talker identification:\
1. two separate independent systems process speech and talker identification (Belin et al. 2004; Gonz\'e1lez and McLennan 2007). This two-systems claim makes the implicit assumption that acoustic cues to word identity and talker identity are completely independent of each other. \
2. both talker identification and language processing arose from evolutionarily earlier capacities to recognize individuals from vocal cues (e.g. Owren and Cardillo 2006), and are thus intertwined. One-system claim suggest that talker identification and language processing utilize the same set of memory representations, and that humans learn (during language acquisition) what elements of sound suggest a difference in meaning, a difference in talker, or both.\
\
We\'92ve known for a long time that speech-sound characteristics and talker characteristics are not independent (Ladefoged and Broadbent 1957). Formants are louder frequency regions present in vowels that differ from vowel to vowel; listeners interpret  formants differently depending on the voice\'92s pitch (Hillenbrand et al. 1995). Importantly, listeners can use formants alone to identify talker gender (Fellowes et al. 1997), suggesting that phoneme representations and talker specific details are closely linked.\
\
Talker information profoundly affects speech processing, so speech processing is probably not truly modular; in that case, talker information should not impinge, but it does, so speech processing is not modular.\
\
Sometimes overlooked is a more straightforward parallel between speech and animal vocalizations: communicating the vocalizer\'92s identity and physical characteristics. (see e.g. Zuberb\'fchler 2002). Homology is the existence of shared ancestry between a pair of structures, or genes, in different species. A common example of this is the wings of bats and the arms of primates. The communication systems of non-human primates may be homologous to human speech.\
\
Birdsong is an important animal model of vocal learning, and is the best-studied non-human analog to speech, primarily because its development parallels the development of human speech perception and production (Doupe and Kuhl 1999). Unlike non-human primates, who do not learn their vocalizations, songbirds are prolific vocal learners, with many species learning new songs throughout life. If isolated or deafened during development, songbirds do not produce normal song (Konishi 1963; Marler and Tamura 1964). Continued auditory feedback is required for normal song to be maintained (Nordeen and Nordeen 1992). Birdsong may include cues to an individual\'92s sex, health and reproductive state, and can often be used to precisely identify another individual (Falls 1982).\
\
Other talker identity source characteristics include jitter (micro-variations in f0) and shimmer (micro-variations in loudness), which in normal speakers decrease as vocal loudness increases.\
\
Different talkers produce different caustic realizations of many phonemes, with variability existing between genders (Simpson 2009) and individual talkers (Fellowes et al. 1997).\
\
Gendered differences may be partly socially conditioned. In children, vocal-tract properties differentiate around puberty (Fitch and Giedd 1999); however, before puberty, female children show higher formant frequencies than males even though F0 does not differ yet (Perry et al. 2001). This suggests that learned \'91gender dialects\'92 (Johnson 2006) may generate differences in male v. female speech patterns. Longer-time-scale properties like F0 variability (prosody) and timing characteristics may also distinguish talkers.\
\
vowel and talker-identifying information are not processed independently. Nonetheless, researchers have continued to search for talker-relevant acoustic properties. The most reliable cues, emerging across all studies, is F0. Others include formant frequencies (Baumann and Belin 2010; Murry and Singh 1980), hoarseness (Murry and Singh; Singh and Murry 1978), vowel duration (Murry and Singh again), and shimmer (Kreiman et al. 1992). However, other than F0, there is little consistency in the cues that are uncovered.\
\
The best current account of talker identification cues is that listeners can exploit myriad acoustic cues to talker identity, and at least some cues also function in speech-sound identification. \
\
-How does the overlap between talker and speech information (talker variability) affect speech processing?\
Since talkers vary in attributes that distinguish speech sounds, we see of course that some acoustic correlates of talker identity affect speech perception.\
\
Talker interference effects: when the talker changes rapidly during an experiment, speech recognition becomes more difficult\
Talker specificity effects: when talker information is consistent (for instance, when a talker who said a word once says it again) recognition may be facilitated\
\
Talker interference effects (harder to recognize whether I just heard /b/ or /p/ when the speaker keeps changing) helps show us that if listeners could pay attention just to the phonemes, then unpredictable talker variation should not distract them. But it did. Listeners were slower and less accurate in identifying speech sounds when talkers varied unpredictably, suggesting that they could not attend just to phonemic information. (Mullennix and Pisoni 1990; Nusbaum and Morin 1992; Green et al. 1997). Memory for word-lists shows talker interference too; recall is worse when the speaker changes (Martin et al. 1989). These authors all thought that cognitive processing space for encoding words was taken up by adjusting to a different talker on each word.\
\
HOWEVER, this effect reverses when time between words is increased (Goldinger et al. 1991), suggesting that more time allows listeners to use talker differences to more richly encode words in memory.\
\
The reason for listener\'92s difficulty in perception when talker changes rapidly seems to be the perceptual or attentional adjustment required, often called talker normalization.\
\
A lead-in phrase calibrates listeners to that speaker\'92s vowel space, consistent with Joos (1948) early account of talker normalization.\
\
One idea is that talker normalization represents not just an adjustment to incoming information, but activation of expectations. Johnson et al 1999 describes studies where the talker\'92s apparent gender influences the perception of speech sounds. This suggests that listeners are normalizing to a memory representation of one group of talkers versus another.\
\
Talker-Specificity Effects:\
processing is better when talker information remains constant from one presentation to the next. Nygaard et al. 1994 showed that listeners understand familiar talkers better than unfamiliar talkers in noise.\
\
But at what level is talker-specific information represented? Creel (2008) found evidence for storage of talker-specific instances of whole words. Eisner and McQueen (2005) have found effects at the sub lexical level.\
\
Infants seem to form word representations that are too acoustically specific. 7.5 month olds cannot distinguish a familiarized word from an unfamiliarized one when it is spoken by a new, dissimilar talker, but they can by 10.5 months.\
\
Interestingly, infants are more accurate at identifying words when they are \'91taught\'92 (exposed to) words in a variety of voices (Rost and McMurray 2009, 2010) or vocal emotions (Singh 2008).\
\
These studies suggest that new listeners have difficulty generalizing across talker variability, which improves with exposure to a wider range of talkers. If you were to think that with more exposure, learners increasingly tune out cues irrelevant to word identity, you would falsely predict that adults would be less adept than children at identifying voices, because adults have tuned out much talker variability, but this doesn\'92t seem to be the case (Bartholomeus 1973, Mann et al 1979).\
\
The alternative (correct?) explanation is that learners improve at tuning in to different dimensions of speech variability, contingent on the listening situation (identifying words v. identifying talkers). Goldinger (1996, 1998) suggests that word and talker information reside in the same exemplar-style representations (Hintzman 1986; Nosofsky 1989) - mental recordings of every speech instance the listener has ever experienced. Storing them all would allow clusters of information - speech sounds, talkers - to emerge naturally. Different clusterings could be focused on by directing attention to particular acoustic attributes. For instance, attention to creaky voice might be higher when trying to recognize a talker v. a word.\
\
Going beyond effects of familiarity, Creel (2010) found evidence for specific, high-level mappings of voice attributes to individuals: preschool-aged children who knew two talkers\'92 favorite colors used talker information early in an instruction sentence (\'91Can you help me find the square?\'92) to visually fixate shapes of the talker\'92s preferred color, prior to hearing the shape name itself. Crucially, they only did so when the talker asked for herself, not when she asked for the other talker. This suggests that, at least in simple cases, children use talker information to constrain sentence interpretation like adults. It is less clear whether children would be able to learn or utilize more subtle or complex information.\
\
Many, if not all cues, can function dually to identify phonemes and talkers. How are talker and speech information stored - as the output of two separate systems, part of a single set of representations (e.g. collections of all experienced speech), or both? Many talker-varying acoustic properties also identify speech sounds, suggesting that the same representations may be needed for both talker and speech-sound identification. Second, studies on talker-specificity effects show that when listeners learn to recognize a particular talker\'92s unusual phoneme, they adjust recognition of other words that contain this phoneme. That is, talker-specific properties affect recognition of speech sounds. If talker information were stored separately from speech-sound information, this should not happen - a distorted phoneme would be useful for identifying the talker as that individual, but would not aid recognition of speech from that talker. Finally, some neuroimaging evidence suggests two separate systems, with speech sounds analyzed in the left brain hemisphere and talker identity in the right\'85 but this hasn\'92t always been shown. Results across studies may differ in the temporal scale of the acoustic information being used to identify speech sounds v. talkers (for instance, VOT versus prosody). This is because the left hemisphere seems biased toward rapid temporal events while the right processes slower events (Zatorre and Belin 2001), meaning that right-hemisphere activation may reflect use of slower temporal scales rather than specialization for talker identification.\
\
That is, listeners distinguish speech sounds by short-time-scale properties, but tend to distinguish voices - especially unfamiliar ones - by slower-time-scale properties. The information to be identified (talker v. speech) and expertise or familiarity with the voices (Perrachione et al. 2009) may affect the temporal scale used, with distinctions among more familiar voices being made from more temporally fine-grained information.\
\
References:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 Baumann and Belin 2010 \'97> formant frequencies as elements of talker individuality identification\
Belin et al. 2004 \'97> two-systems hypothesis of speech perception (bullcrap)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 Bricker and Pruzansky 1966 \'97> first time knowing speech-sound characteristics and talker characteristics are not independent\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 Doupe and Kuhl 1999 \'97> birdsong development parallels human speech perception and production\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 Fellowes et al. 1997 \'97> variability in vowels between different individual speakers\
Goldinger et al. 1991 \'97> reverse of talker interference when time between words is increased\
Goldinger (1998): Echoes of echoes? An episodic theory of lexical access\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 Gonz\'e1lez and McLennan 2007 \'97> more two-systems hypothesis\
Green et al. 1997 \'97> talker interference effects\
Hillenbrand et al. 1995 \'97> formants interpreted differently depending on pitch\
Houston and Jusczyk 2000 \'97> children form word representations that are two acoustically specific\
Johnson et al 1999. Auditory-visual integration of talker gender in vowel perception.\
Konishi 1963 \'97> songbirds not producing normal song if deafened or isolated\
Kreiman et al. 1992 \'97> shimmer in identifying talkers\
Marler and Tamura 1964 \'97> songbirds not producing normal song if deafened or isolated\
Martin et al. 1989 \'97> talker interference with word list recall\
Mullennix and Pisoni 1990 \'97> talker interference effects\
Murry and Singh 1980 \'97> formant frequencies, hoarseness, vowel duration in identifying talkers\
Nordeen and Nordeen 1992 \'97> continued auditory feedback required for normal songbird song\
Nusbaum and Morin 1992 \'97> talker interference effects\
Owren and Cardillo 2006 \'97> unified system hypothesis of speech perception***\
Perry et al. 2001 \'97> prepubescent gender differentiation in higher formant frequencies\
Simpson 2009 \'97> variability in vowels between genders\
Singh and Murry 1978 \'97> hoarseness in identifying talkers\
Zuberb\'fchler 2002 \'97> communicating speaker identity in birdsong}