{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 Event-related brain responses revealed that the speaker\'92s identity is taken into account as early as 200-300 msec after the beginning of a spoken word, and is processed by the same early interpretation mechanism that constructs sentence meaning based on just the words.\
\
This finding is difficult to reconcile with standard \'93Gricean\'94 models of sentence interpretation in which comprehends initially compute a local, context-independent meaning for the sentence (\'93semantics\'94) before working out what it really means given the wider communicative context and the particular speaker (\'93pragmatics\'94).\
\
Because the observed brain response (N400s) hinges on voice-based and usually stereotype-dependent inferences about the speaker, it also shows that listeners rapidly classify speakers on the basis of their voices and bring the associated social stereotypes to bear on what is being said. According to our event-related potential results, language comprehension takes very rapid account of the social context, and the construction of meaning based on language alone cannot be separated from the social aspects of language use. The linguistic brain relates the message to the speaker immediately.\
\
In psycholinguistics, assuming a Gricean standpoint has led to an analysis that has evolved into the standard two-step model of language interpretation. In this model, listeners (and readers) first compute a local, context-independent meaning for the sentence, and only then work out what it really means given the wider communicative context and the particular speaker (Lattner \\& Friederici, 2003; Cutler \\& Clifton, 1999; Sperber \\& Wilson, 1995; Fodor, 1983; Grice, 1975). 
\b Mismatches between message and speaker would be detected in the second step only, in slow pragmatic computations that are different from the rapid semantic computations in which word meanings are combined (e.g., Lattner \\& Friederici, 2003). From a design perspective, however, it would make sense for the linguistic brain to take the speaker into account right from the start. After all, human language has evolved to support social, interpersonal interaction.\
\

\b0 Also, recent linguistic research suggests that the computation of a context-free sentence meaning is, in fact, highly problematic, and that linguistic meaning is always colored by the pragmatics of the communicative exchange (Kempson, 2001; Perry, 1997; Clark, 1996). The meaning of so-called indexicals such as \'93I\'94 and \'93you,\'94 for example, inevitably depends on the communicative situation (e.g., Perry, 1997), and upon closer analysis, so does the meaning of apparently self-sufficient words such as \'93garage\'94 or \'93Kensington Gardens\'94 (Kempson, 2001; Clark, 1996).\
\
These analyses are at odds with the standard two-step model of interpretation. Instead, they suggest a one-step model in which knowledge about the speaker is brought to bear immediately by the same fast-acting brain system that combines the meanings of individual words into a larger whole.\
\
Hanna and Tanenhaus (2004) showed that listeners who were asked to hand over something to the speaker were rapidly sensitive to whether the latter could have picked up the item himself or herself. Such eye tracking findings show that, in conversation, listeners rapidly relate the message to characteristics of the speaker.\
\
\pard\pardeftab720\sa240
\cf0 The critical prediction of this model is that because semantic information provided by the words in a sentence and voice- conveyed information about the identity of the speaker are handled by the same early sense-making process, semantic anomalies and speaker inconsistencies will generate the same type of ERP effect, an N400 effect, doing so in the typical latency range for N400 amplitude modulations. The two-step model of semantic interpretation makes a different prediction: If contextual information about the speaker is handled in a dis- tinct second phase of interpretation, then speaker inconsistencies should elicit a delayed and possibly quite different ERP effect.\
According to our ERP results, the brain integrates message and speaker very rapidly, within some 200-300 msec after the acoustic onset of a relevant word. Also, speaker inconsistencies elicited the same type of brain response as semantic anomalies, an N400 effect. That is, voice-inferred information about the speaker is taken into account by the same early language interpretation mechanisms that construct ``sentence-internal\'94 meaning based on just the words. Our findings therefore demonstrate that, as far as the brain is concerned, linguistic meaning depends on the pragmatics of the communicative situation right from the start. However, by revealing an equally immediate impact of what listeners infer about the speaker, the present results add a distinctly social dimension to the mechanisms of on-line language interpretation. Language users very rapidly model the speaker to help determine what is being said. This makes sense, as language evolved in face-to-face social interaction and, importantly, requires close coordination among interlocutors (Clark, 1996).
\f1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0 \cf0 The ERP data reported here show that listeners use what they know or infer about the speaker from his or her voice in the earliest stages of meaning construction. This suggests that, to the brain of the language user, there is no context-free meaning. Instead, according to these findings, sentence interpretation is an intrinsically contextualized social activity (Kempson, 2001; Clark, 1996). Our results are difficult to reconcile with two-step \'93Gricean\'94 models of sentence interpretation based on the classic distinction between semantics and pragmatics. \
\
As such, they could also be taken to query the nature of the distinction itself. Of course, the implication is not that listeners cannot tell the difference between a message and a speaker, and that everything blurs into an undifferentiated whole \'97 the fact that we can perceive and reflect upon the conflict in a male voice uttering \'93I think I am pregnant\'94 reveals that we can recover the lexically coded part of the message regardless of who the speaker is. However, what the brain\'92s electrophysiology allows us to see is that voice-based inferences about the identity of the speaker and information encoded in the meaning of spoken words jointly constrain the same early sense-making process, without a principled delay in the use of speaker-related information. The linguistic brain is not just combining words in a context-free semantic universe confined in a single person\'92s skull. It immediately cares about other people.\
 \
References:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 Hanna and Tanenhaus (2004) \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 Kempson 2001\
Perry 1997\
Clark 1996}